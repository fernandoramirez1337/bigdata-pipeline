# Log de ImplementaciÃ³n - Big Data Pipeline AWS

**Fecha de inicio**: 20 de Noviembre, 2025
**Cluster**: 4 EC2 instances en AWS Free Tier
**Objetivo**: Pipeline completo de Big Data para NYC Taxi Dataset (165M registros)

---

## Tabla de Contenidos

1. [Resumen Ejecutivo](#resumen-ejecutivo)
2. [Paso a Paso Completado](#paso-a-paso-completado)
3. [Problemas Encontrados y Soluciones](#problemas-encontrados-y-soluciones)
4. [Estado Actual del Cluster](#estado-actual-del-cluster)
5. [PrÃ³ximos Pasos](#prÃ³ximos-pasos)
6. [ConfiguraciÃ³n Final del Cluster](#configuraciÃ³n-final-del-cluster)

---

## Resumen Ejecutivo

### âœ… Completado

- **Infraestructura EC2**: 4 instancias creadas y configuradas
- **Networking**: Security Group, /etc/hosts, conectividad SSH
- **Scripts de automatizaciÃ³n**: Actualizados para AWS Free Tier
- **InstalaciÃ³n base**: Java 11 Amazon Corretto, Python, utilidades
- **Fix crÃ­tico**: Resuelto conflicto de paquete curl en Amazon Linux 2023

### â³ En Progreso

- **InstalaciÃ³n de software distribuido**: Kafka, Zookeeper, Flink, Spark, HDFS, PostgreSQL, Superset
- **Proceso actual**: `./orchestrate-cluster.sh setup-all` ejecutÃ¡ndose (~30-45 minutos)

### ğŸ“‹ Pendiente

- InicializaciÃ³n de servicios
- ConfiguraciÃ³n de Kafka topics
- Despliegue de jobs Flink y Spark
- ConfiguraciÃ³n de Superset dashboards
- Ingesta de datos NYC Taxi

---

## Paso a Paso Completado

### Fase 1: PreparaciÃ³n de Infraestructura âœ…

#### 1.1 CreaciÃ³n del Cluster EC2 (14:40 UTC)

```bash
cd bigdata-pipeline/infrastructure/scripts
./create-cluster.sh
```

**Resultado**: 4 instancias EC2 creadas exitosamente en AWS Free Tier

| Instancia | ID | Tipo | IP Privada | IP PÃºblica | Volumen EBS |
|-----------|-----|------|-----------|------------|-------------|
| Master | i-095692309cc67a77e | t3.small | 172.31.72.49 | 44.210.18.254 | 30 GB |
| Worker1 | i-09a02241a47abc50d | c7i-flex.large | 172.31.70.167 | 44.221.77.132 | 30 GB |
| Worker2 | i-0d0535e69242b7db5 | c7i-flex.large | 172.31.15.51 | 3.219.215.11 | 30 GB |
| Storage | i-05ebe42a3698b6b71 | m7i-flex.large | 172.31.31.171 | 98.88.249.180 | 30 GB |

**Componentes por instancia**:
- **Master**: Kafka, Zookeeper, Flink JobManager, Spark Master, HDFS NameNode
- **Worker1**: Flink TaskManager, Spark Worker, HDFS DataNode
- **Worker2**: Flink TaskManager, Spark Worker, HDFS DataNode
- **Storage**: PostgreSQL, Superset, HDFS DataNode, S3 connector

**Tipos de instancia**:
- t3.small: 2 vCPUs, 2 GB RAM (Master - coordinaciÃ³n)
- c7i-flex.large: 2 vCPUs, 4 GB RAM (Workers - procesamiento)
- m7i-flex.large: 2 vCPUs, 8 GB RAM (Storage - base de datos)

#### 1.2 ConfiguraciÃ³n de Red âœ…

**Security Group**: `bigdata-cluster-sg` creado con reglas:

| Puerto(s) | Protocolo | Origen | Servicio |
|-----------|-----------|--------|----------|
| 22 | TCP | IP del usuario | SSH |
| 2181 | TCP | VPC | Zookeeper |
| 5432 | TCP | VPC | PostgreSQL |
| 6123 | TCP | VPC | Flink RPC |
| 7077 | TCP | VPC | Spark Master |
| 8080-8088 | TCP | 0.0.0.0/0 | Web UIs (Spark, Flink, Superset) |
| 9000 | TCP | VPC | HDFS NameNode RPC |
| 9092 | TCP | VPC | Kafka Broker |
| 9870 | TCP | 0.0.0.0/0 | HDFS Web UI |
| All | All | Security Group | ComunicaciÃ³n inter-cluster |

#### 1.3 ConfiguraciÃ³n de /etc/hosts âœ…

Ejecutado en las 4 instancias:

```bash
sudo tee -a /etc/hosts <<EOF
172.31.72.49  master-node bigdata-master
172.31.70.167 worker1-node bigdata-worker1
172.31.15.51  worker2-node bigdata-worker2
172.31.31.171 storage-node bigdata-storage
EOF
```

**VerificaciÃ³n**: Ping exitoso entre todas las instancias usando hostnames.

#### 1.4 ActualizaciÃ³n de orchestrate-cluster.sh âœ…

Archivo actualizado con IPs pÃºblicas para SSH desde mÃ¡quina local:

```bash
# infrastructure/scripts/orchestrate-cluster.sh
MASTER_IP="44.210.18.254"
WORKER1_IP="44.221.77.132"
WORKER2_IP="3.219.215.11"
STORAGE_IP="98.88.249.180"
```

**Nota importante**: Se usan IPs pÃºblicas para SSH desde local, pero los servicios internos usan IPs privadas.

---

### Fase 2: InstalaciÃ³n de Software Base âœ…

#### 2.1 Primer Intento de InstalaciÃ³n (14:42 UTC) âŒ

**Comando ejecutado**:
```bash
./orchestrate-cluster.sh setup-all
```

**Resultado**: FALLO - Error de paquete curl

**Error encontrado** (en las 4 instancias):
```
Error:
 Problem: problem with installed package curl-minimal-8.5.0-1.amzn2023.0.4.x86_64
  - package curl-minimal-8.5.0-1.amzn2023.0.4.x86_64 from @System conflicts
    with curl provided by curl-7.87.0-2.amzn2023.0.2.x86_64 from amazonlinux
  - conflicting requests
```

**AnÃ¡lisis del problema**:
- Amazon Linux 2023 viene con `curl-minimal` pre-instalado
- El script `common-setup.sh` intentaba instalar el paquete completo `curl`
- Ambos paquetes proveen funcionalidad curl pero son mutuamente excluyentes
- El instalador de paquetes (dnf) no puede resolver este conflicto automÃ¡ticamente

**Impacto**:
- InstalaciÃ³n bloqueada en paso [3/8] "Installing essential utilities"
- Todos los pasos subsecuentes no se ejecutaron
- Las 4 instancias quedaron con instalaciÃ³n parcial:
  - âœ… Java 11 Amazon Corretto instalado
  - âœ… Variables de entorno configuradas
  - âŒ Utilidades esenciales: instalaciÃ³n incompleta
  - âŒ Directorios y configuraciones: no creados
  - âŒ Software Big Data: no instalado

#### 2.2 ResoluciÃ³n del Problema (14:55 UTC) âœ…

**Archivo modificado**: `infrastructure/scripts/common-setup.sh`

**Cambio realizado** (lÃ­nea 44):
```diff
 # Instalar utilidades esenciales
 echo -e "${GREEN}[3/8] Installing essential utilities...${NC}"
 sudo yum install -y \
     wget \
-    curl \
     tar \
     gzip \
     git \
```

**JustificaciÃ³n**:
- `wget` ya estÃ¡ siendo instalado y provee funcionalidad similar a curl
- `curl-minimal` ya estÃ¡ pre-instalado en Amazon Linux 2023
- Remover curl del script elimina el conflicto sin pÃ©rdida de funcionalidad

**Commit**:
```bash
git commit -m "Fix curl package conflict on Amazon Linux 2023"
git push -u origin claude/aws-ec2-distributed-plan-018mA2bSY4uvxcLBuHq1CkDS
```

Commit ID: `4099a6b`

#### 2.3 Segunda EjecuciÃ³n - InstalaciÃ³n Exitosa (14:57 UTC) â³

**Comando ejecutado**:
```bash
git pull origin claude/aws-ec2-distributed-plan-018mA2bSY4uvxcLBuHq1CkDS
./orchestrate-cluster.sh setup-all
```

**Progreso de instalaciÃ³n**:

##### Common Setup (Todas las instancias) âœ…

**Step [1/8] - System Update**: âœ… Completado
- Paquetes del sistema actualizados
- Advertencia: VersiÃ³n mÃ¡s reciente de Amazon Linux disponible (no crÃ­tico)

**Step [2/8] - Java Installation**: âœ… Completado
- Instalado: `java-11-amazon-corretto-1:11.0.25+9-1.amzn2023.x86_64`
- Instalado: `java-11-amazon-corretto-devel`
- JAVA_HOME configurado: `/usr/lib/jvm/java-11-amazon-corretto`
- VerificaciÃ³n: OpenJDK 11.0.25 funcionando correctamente

**Step [3/8] - Essential Utilities**: âœ… Completado
```
Instalados:
- git 2.40.1
- htop 3.2.1
- nmap-ncat (nc)
- python3-pip 21.3.1
- telnet
- tmux 3.2a
- wget, tar, gzip, vim, net-tools, python3 (ya estaban instalados)
```

**Step [4/8] - Directory Creation**: âœ… Completado
```
Directorios creados:
- /opt/bigdata (instalaciÃ³n de software)
- /data/kafka, /data/zookeeper, /data/hdfs, /data/flink, /data/spark, /data/postgresql
- /var/log/bigdata (logs centralizados)
```

**Step [5/8] - System Limits**: âœ… Completado
```
Configurado en /etc/security/limits.conf:
- nofile (archivos abiertos): 65536 soft/hard
- nproc (procesos): 32768 soft/hard
```

**Step [6/8] - Transparent Huge Pages**: âœ… Completado
```
Deshabilitado THP para mejor rendimiento en Big Data:
- /sys/kernel/mm/transparent_hugepage/enabled: never
- /sys/kernel/mm/transparent_hugepage/defrag: never
```

**Step [7/8] - Swappiness**: âœ… Completado
```
vm.swappiness configurado a 10 (reducir uso de swap)
```

**Step [8/8] - Python Packages**: âœ… Completado
```
Instalados:
- pip 25.3 (actualizado desde 21.3.1)
- kafka-python 2.0.2
- pandas 2.0.3
- pyarrow 12.0.1
- boto3 1.28.25 (AWS SDK)
- psycopg2-binary 2.9.7 (PostgreSQL driver)
- pyyaml 6.0.1
- numpy 2.0.2 (dependencia de pandas)
- python-dateutil 2.9.0.post0
```

**Advertencia menor**:
```
ERROR: pip's dependency resolver does not currently take into account all
the packages that are installed. This behaviour is the source of the
following dependency conflicts.
awscli 2.15.30 requires python-dateutil<=2.8.2,>=2.1, but you have
python-dateutil 2.9.0.post0 which is incompatible.
```
- **Impacto**: Bajo - AWS CLI no es crÃ­tico para el pipeline
- **AcciÃ³n**: No requiere correcciÃ³n inmediata

##### Master Node Setup â³ EN PROGRESO

**[1/6] - Apache Zookeeper 3.8.3**: âœ… Completado (2 minutos)
```bash
wget https://archive.apache.org/dist/zookeeper/zookeeper-3.8.3/apache-zookeeper-3.8.3-bin.tar.gz
# Descargado: 14.8 MB
# Velocidad promedio: 120 KB/s
```

**[2/6] - Apache Kafka 3.6.0**: â³ EN PROGRESO (~7% completado)
```bash
wget https://archive.apache.org/dist/kafka/3.6.0/kafka_2.13-3.6.0.tgz
# TamaÃ±o total: 108 MB
# Velocidad promedio: ~150 KB/s
# Tiempo estimado: 11-12 minutos
```

**Pendiente en Master**:
- [3/6] Apache Flink 1.18.0
- [4/6] Apache Spark 3.5.0
- [5/6] Hadoop HDFS 3.3.6
- [6/6] Configuraciones y servicios

##### Worker Nodes Setup â¸ï¸ ESPERANDO

Setup de Worker1 y Worker2 iniciarÃ¡ despuÃ©s de completar Master.

**Componentes a instalar**:
- Apache Flink 1.18.0 (TaskManager)
- Apache Spark 3.5.0 (Worker)
- Hadoop HDFS 3.3.6 (DataNode)

##### Storage Node Setup â¸ï¸ ESPERANDO

Setup de Storage iniciarÃ¡ despuÃ©s de completar Workers.

**Componentes a instalar**:
- PostgreSQL 15
- Apache Superset 3.1.0
- Hadoop HDFS 3.3.6 (DataNode)
- AWS CLI v2
- Scripts de sincronizaciÃ³n S3

---

## Problemas Encontrados y Soluciones

### 1. Conflicto de Paquete curl âŒ â†’ âœ…

**Problema**:
```
Error: package curl-minimal conflicts with curl
```

**Causa RaÃ­z**:
- Amazon Linux 2023 incluye `curl-minimal` por defecto
- El script intentaba instalar `curl` completo
- DNF no puede tener ambos paquetes simultÃ¡neamente

**SoluciÃ³n Implementada**:
- Removido `curl` de la lista de instalaciÃ³n en `common-setup.sh`
- `wget` provee funcionalidad equivalente para descargas
- `curl-minimal` ya disponible para operaciones bÃ¡sicas

**Archivos modificados**:
- `infrastructure/scripts/common-setup.sh` (lÃ­nea 44)

**Commit**: 4099a6b

**Estado**: âœ… RESUELTO

### 2. Velocidad de Descarga Lenta âš ï¸

**ObservaciÃ³n**:
- Descargas de Apache Archive a ~120-150 KB/s
- Zookeeper (14.8 MB): ~2 minutos
- Kafka (108 MB): ~11-12 minutos estimados

**Causa**:
- Limitaciones de ancho de banda de instancias t3.small/c7i-flex.large
- Apache Archive puede tener throttling
- Latencia entre AWS us-east-1 y servidores de Apache

**Impacto**:
- Proceso de instalaciÃ³n completo tardarÃ¡ 30-45 minutos
- No afecta funcionamiento posterior del cluster
- Solo impacta tiempo de setup inicial

**MitigaciÃ³n**:
- Considerar para futuras implementaciones:
  - Pre-descargar archives y subirlos a S3
  - Usar AMI personalizada con software pre-instalado
  - Aprovechar mirrors mÃ¡s cercanos geogrÃ¡ficamente

**Estado**: âš ï¸ ACEPTADO (no crÃ­tico)

---

## Estado Actual del Cluster

### Timestamp: 20 Nov 2025, 15:05 UTC

#### Resumen de Estado

| Componente | Master | Worker1 | Worker2 | Storage | Estado |
|------------|--------|---------|---------|---------|--------|
| **Common Setup** | âœ… | âœ… | âœ… | âœ… | Completado |
| Java 11 | âœ… | âœ… | âœ… | âœ… | Instalado |
| Python 3.9 + packages | âœ… | âœ… | âœ… | âœ… | Instalado |
| Directorios | âœ… | âœ… | âœ… | âœ… | Creados |
| **Specific Setup** | â³ | â¸ï¸ | â¸ï¸ | â¸ï¸ | En progreso |
| Zookeeper | âœ… | - | - | - | Descargado |
| Kafka | â³ | - | - | - | Descargando |
| Flink | â¸ï¸ | â¸ï¸ | â¸ï¸ | - | Pendiente |
| Spark | â¸ï¸ | â¸ï¸ | â¸ï¸ | - | Pendiente |
| HDFS | â¸ï¸ | â¸ï¸ | â¸ï¸ | â¸ï¸ | Pendiente |
| PostgreSQL | - | - | - | â¸ï¸ | Pendiente |
| Superset | - | - | - | â¸ï¸ | Pendiente |

#### Conectividad y Acceso

**SSH desde local**: âœ… Funcionando
```bash
ssh -i ~/.ssh/bigd-key.pem ec2-user@44.210.18.254  # Master
ssh -i ~/.ssh/bigd-key.pem ec2-user@44.221.77.132  # Worker1
ssh -i ~/.ssh/bigd-key.pem ec2-user@3.219.215.11   # Worker2
ssh -i ~/.ssh/bigd-key.pem ec2-user@98.88.249.180  # Storage
```

**ResoluciÃ³n de nombres internos**: âœ… Configurado
```bash
ping master-node    # â†’ 172.31.72.49
ping worker1-node   # â†’ 172.31.70.167
ping worker2-node   # â†’ 172.31.15.51
ping storage-node   # â†’ 172.31.31.171
```

#### Recursos del Sistema

| Instancia | vCPUs | RAM | Disco | Uso RAM | Disco Libre |
|-----------|-------|-----|-------|---------|-------------|
| Master | 2 | 1.9 GB | 30 GB | ~300 MB | ~28 GB |
| Worker1 | 2 | 4 GB | 30 GB | ~300 MB | ~28 GB |
| Worker2 | 2 | 4 GB | 30 GB | ~300 MB | ~28 GB |
| Storage | 2 | 8 GB | 30 GB | ~300 MB | ~28 GB |

---

## PrÃ³ximos Pasos

### Inmediato (En las prÃ³ximas 2 horas)

#### 1. Completar InstalaciÃ³n AutomÃ¡tica â³

**AcciÃ³n**: Esperar a que termine `./orchestrate-cluster.sh setup-all`

**Componentes pendientes de instalaciÃ³n**:

**Master Node**:
- âœ… Zookeeper 3.8.3
- â³ Kafka 3.6.0 (en progreso)
- â¸ï¸ Flink 1.18.0 JobManager
- â¸ï¸ Spark 3.5.0 Master
- â¸ï¸ Hadoop HDFS 3.3.6 NameNode

**Worker1 & Worker2**:
- â¸ï¸ Flink 1.18.0 TaskManager
- â¸ï¸ Spark 3.5.0 Worker
- â¸ï¸ Hadoop HDFS 3.3.6 DataNode

**Storage Node**:
- â¸ï¸ PostgreSQL 15
- â¸ï¸ Apache Superset 3.1.0
- â¸ï¸ Hadoop HDFS 3.3.6 DataNode

**Tiempo estimado**: 25-35 minutos adicionales

#### 2. Verificar InstalaciÃ³n Exitosa âœ…

Cuando termine la instalaciÃ³n, verificar:

```bash
# Verificar versiones instaladas en Master
ssh ec2-user@44.210.18.254
java -version                                    # Java 11
ls /opt/bigdata/                                 # Directorios de software
ls /opt/bigdata/zookeeper-*                     # Zookeeper instalado
ls /opt/bigdata/kafka-*                         # Kafka instalado
ls /opt/bigdata/flink-*                         # Flink instalado
ls /opt/bigdata/spark-*                         # Spark instalado
ls /opt/bigdata/hadoop-*                        # HDFS instalado
```

```bash
# Verificar Workers
ssh ec2-user@44.221.77.132
ls /opt/bigdata/                                # Software instalado
```

```bash
# Verificar Storage
ssh ec2-user@98.88.249.180
psql --version                                  # PostgreSQL instalado
superset version                                # Superset instalado
```

#### 3. Iniciar Servicios del Cluster ğŸš€

**OpciÃ³n A - Usando orchestrate-cluster.sh (Recomendado)**:
```bash
# Desde mÃ¡quina local
cd bigdata-pipeline/infrastructure/scripts
./orchestrate-cluster.sh start-cluster
```

**OpciÃ³n B - Manual**:

En **Master**:
```bash
ssh ec2-user@44.210.18.254

# 1. Iniciar Zookeeper
/opt/bigdata/zookeeper-3.8.3/bin/zkServer.sh start

# 2. Iniciar Kafka
/opt/bigdata/kafka-3.6.0/bin/kafka-server-start.sh -daemon \
  /opt/bigdata/kafka-3.6.0/config/server.properties

# 3. Iniciar HDFS NameNode
/opt/bigdata/hadoop-3.3.6/bin/hdfs --daemon start namenode

# 4. Iniciar Flink JobManager
/opt/bigdata/flink-1.18.0/bin/jobmanager.sh start

# 5. Iniciar Spark Master
/opt/bigdata/spark-3.5.0/sbin/start-master.sh
```

En **Worker1 & Worker2**:
```bash
# Para cada worker
ssh ec2-user@<WORKER_IP>

# 1. Iniciar HDFS DataNode
/opt/bigdata/hadoop-3.3.6/bin/hdfs --daemon start datanode

# 2. Iniciar Flink TaskManager
/opt/bigdata/flink-1.18.0/bin/taskmanager.sh start

# 3. Iniciar Spark Worker
/opt/bigdata/spark-3.5.0/sbin/start-worker.sh spark://master-node:7077
```

En **Storage**:
```bash
ssh ec2-user@98.88.249.180

# 1. Iniciar PostgreSQL
sudo systemctl start postgresql
sudo systemctl enable postgresql

# 2. Iniciar HDFS DataNode
/opt/bigdata/hadoop-3.3.6/bin/hdfs --daemon start datanode

# 3. Iniciar Superset
superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger &
```

#### 4. Verificar Estado de Servicios âœ…

**Comando automatizado**:
```bash
./orchestrate-cluster.sh status
```

**VerificaciÃ³n manual**:

```bash
# En Master
ssh ec2-user@44.210.18.254
jps  # DeberÃ­a mostrar: QuorumPeerMain, Kafka, NameNode, StandaloneSession, Master
```

**Verificar Web UIs** (desde navegador):
- HDFS NameNode: http://44.210.18.254:9870
- Spark Master: http://44.210.18.254:8080
- Flink Dashboard: http://44.210.18.254:8081
- Superset: http://98.88.249.180:8088

---

### Corto Plazo (Siguientes 4-8 horas)

#### 5. Crear Kafka Topics ğŸ“¨

```bash
./orchestrate-cluster.sh create-topics
```

O manualmente:
```bash
ssh ec2-user@44.210.18.254
/opt/bigdata/kafka-3.6.0/bin/kafka-topics.sh --create \
  --bootstrap-server master-node:9092 \
  --topic taxi-trips \
  --partitions 3 \
  --replication-factor 1

# Verificar
/opt/bigdata/kafka-3.6.0/bin/kafka-topics.sh --list \
  --bootstrap-server master-node:9092
```

#### 6. Configurar PostgreSQL ğŸ—„ï¸

En Storage node:
```bash
ssh ec2-user@98.88.249.180

# Crear base de datos y usuario
sudo -u postgres psql <<EOF
CREATE DATABASE taxi_analytics;
CREATE USER bigdata WITH PASSWORD 'bigdata123';
GRANT ALL PRIVILEGES ON DATABASE taxi_analytics TO bigdata;
\q
EOF

# Crear tablas para streaming results
sudo -u postgres psql -d taxi_analytics <<EOF
CREATE TABLE zone_metrics (
    zone_id INTEGER,
    window_start TIMESTAMP,
    window_end TIMESTAMP,
    trip_count INTEGER,
    total_amount DECIMAL(10,2),
    avg_distance DECIMAL(10,2),
    avg_passengers DECIMAL(5,2),
    PRIMARY KEY (zone_id, window_start)
);

CREATE INDEX idx_zone_metrics_time ON zone_metrics(window_start, window_end);
CREATE INDEX idx_zone_metrics_zone ON zone_metrics(zone_id);
EOF
```

#### 7. Inicializar Superset ğŸ¨

```bash
ssh ec2-user@98.88.249.180

# Crear usuario admin
superset fab create-admin \
  --username admin \
  --firstname Admin \
  --lastname User \
  --email admin@bigdata.com \
  --password admin123

# Inicializar base de datos
superset db upgrade

# Cargar ejemplos (opcional)
superset load_examples

# Inicializar roles
superset init
```

Acceder: http://98.88.249.180:8088
- Usuario: `admin`
- Password: `admin123`

#### 8. Descargar y Preparar Dataset ğŸ“Š

**Dataset**: NYC Yellow Taxi Trip Data 2015

**OpciÃ³n A - Descargar en Master**:
```bash
ssh ec2-user@44.210.18.254
mkdir -p /data/taxi-dataset

# Descargar 12 meses (2015)
cd /data/taxi-dataset
for month in {01..12}; do
  wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-${month}.parquet
done

# Verificar
ls -lh /data/taxi-dataset/
# DeberÃ­a mostrar 12 archivos .parquet (~40-60 GB total)
```

**OpciÃ³n B - Usar S3 (si ya estÃ¡ disponible)**:
```bash
# Configurar AWS CLI
aws configure

# Copiar desde S3
aws s3 sync s3://nyc-tlc/trip_data/ /data/taxi-dataset/ \
  --exclude "*" \
  --include "yellow_tripdata_2015-*.parquet"
```

---

### Mediano Plazo (Siguientes 1-2 dÃ­as)

#### 9. Compilar y Desplegar Flink Jobs ğŸ”„

**En Master**:
```bash
ssh ec2-user@44.210.18.254
cd ~/bigdata-pipeline/streaming/flink-jobs

# Compilar con Maven
mvn clean package

# Desplegar job
/opt/bigdata/flink-1.18.0/bin/flink run \
  target/taxi-streaming-jobs-1.0-SNAPSHOT.jar \
  --kafka-brokers master-node:9092 \
  --kafka-topic taxi-trips \
  --postgres-host storage-node \
  --postgres-db taxi_analytics \
  --postgres-user bigdata \
  --postgres-password bigdata123
```

**Verificar job en Flink UI**: http://44.210.18.254:8081

#### 10. Configurar Spark Batch Jobs âš¡

**Copiar jobs a cluster**:
```bash
# Desde mÃ¡quina local
scp -i ~/.ssh/bigd-key.pem -r batch/spark-jobs/ \
  ec2-user@44.210.18.254:~/bigdata-pipeline/batch/
```

**Configurar cron para ejecuciÃ³n diaria**:
```bash
ssh ec2-user@44.210.18.254
crontab -e

# Agregar job diario a las 2 AM
0 2 * * * /opt/bigdata/spark-3.5.0/bin/spark-submit \
  --master spark://master-node:7077 \
  --deploy-mode cluster \
  ~/bigdata-pipeline/batch/spark-jobs/daily_summary.py \
  --date $(date -d "yesterday" +\%Y-\%m-\%d) \
  --input hdfs://master-node:9000/taxi/raw/ \
  --output hdfs://master-node:9000/taxi/processed/ \
  >> /var/log/bigdata/spark-daily.log 2>&1
```

#### 11. Implementar Data Producer ğŸ“¡

**En Master**:
```bash
ssh ec2-user@44.210.18.254
cd ~/bigdata-pipeline/data-producer

# Instalar dependencias (ya instaladas en common-setup)
pip3 install -r requirements.txt

# Configurar producer
vim config.yaml
# Actualizar:
#   kafka_brokers: ["master-node:9092"]
#   dataset_path: "/data/taxi-dataset/"
#   replay_speed: 100  # 100x mÃ¡s rÃ¡pido que tiempo real

# Ejecutar producer en background
nohup python3 src/producer.py --config config.yaml \
  > /var/log/bigdata/producer.log 2>&1 &

# Verificar logs
tail -f /var/log/bigdata/producer.log
```

#### 12. Configurar Dashboards en Superset ğŸ“ˆ

1. Acceder a Superset: http://98.88.249.180:8088
2. Crear Database Connection a PostgreSQL
3. Crear Datasets desde tablas
4. Crear Charts:
   - Line Chart: Viajes por minuto
   - Bar Chart: Top 10 zonas
   - Pie Chart: MÃ©todos de pago
   - Big Number: Total ingresos en tiempo real
5. Crear Dashboard combinando charts

---

### Largo Plazo (PrÃ³xima semana)

#### 13. OptimizaciÃ³n y Monitoreo ğŸ“Š

- Configurar alertas en Superset
- Implementar logging centralizado
- Configurar backups de PostgreSQL
- Optimizar configuraciones de Spark/Flink basado en uso real
- Configurar auto-scaling (si se requiere)

#### 14. AnÃ¡lisis Avanzados ğŸ”¬

- Implementar ML models con Spark MLlib
- Crear anÃ¡lisis de series temporales
- Implementar detecciÃ³n de anomalÃ­as
- Generar reportes automatizados

#### 15. DocumentaciÃ³n Final ğŸ“

- Screenshots de dashboards
- MÃ©tricas de rendimiento reales
- Lecciones aprendidas
- GuÃ­a de mantenimiento

---

## ConfiguraciÃ³n Final del Cluster

### Arquitectura de Servicios

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS VPC (us-east-1)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Master (172.31.72.49)       Worker1 (172.31.70.167)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Zookeeper :2181  â”‚        â”‚ Flink TM  :6121  â”‚             â”‚
â”‚  â”‚ Kafka     :9092  â”‚        â”‚ Spark W   :7077  â”‚             â”‚
â”‚  â”‚ Flink JM  :8081  â”‚        â”‚ HDFS DN   :9866  â”‚             â”‚
â”‚  â”‚ Spark M   :8080  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚  â”‚ HDFS NN   :9870  â”‚                                          â”‚
â”‚  â”‚ Data Prod :9999  â”‚        Worker2 (172.31.15.51)           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚                               â”‚ Flink TM  :6121  â”‚             â”‚
â”‚  Storage (172.31.31.171)     â”‚ Spark W   :7077  â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ HDFS DN   :9866  â”‚             â”‚
â”‚  â”‚ PostgreSQL :5432 â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚  â”‚ Superset   :8088 â”‚                                          â”‚
â”‚  â”‚ HDFS DN    :9866 â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### URLs de Acceso

| Servicio | URL | Credenciales |
|----------|-----|--------------|
| HDFS NameNode | http://44.210.18.254:9870 | - |
| Spark Master | http://44.210.18.254:8080 | - |
| Flink Dashboard | http://44.210.18.254:8081 | - |
| Superset | http://98.88.249.180:8088 | admin/admin123 |
| PostgreSQL | storage-node:5432 | bigdata/bigdata123 |

### Comandos Ãštiles

#### Iniciar/Detener Cluster Completo

```bash
# Desde mÃ¡quina local
cd bigdata-pipeline/infrastructure/scripts

# Iniciar todo
./orchestrate-cluster.sh start-cluster

# Detener todo
./orchestrate-cluster.sh stop-cluster

# Ver estado
./orchestrate-cluster.sh status
```

#### Monitorear Logs

```bash
# Master - Kafka
ssh ec2-user@44.210.18.254
tail -f /opt/bigdata/kafka-3.6.0/logs/server.log

# Master - Flink
tail -f /opt/bigdata/flink-1.18.0/log/flink-*-standalonesession-*.log

# Workers - TaskManager
ssh ec2-user@44.221.77.132
tail -f /opt/bigdata/flink-1.18.0/log/flink-*-taskexecutor-*.log

# Storage - PostgreSQL
ssh ec2-user@98.88.249.180
sudo tail -f /var/lib/pgsql/15/data/log/postgresql-*.log
```

#### Verificar Procesos Java

```bash
# En cualquier nodo
jps -l
# Muestra todos los procesos Java con nombres completos
```

#### Reiniciar Servicios Individuales

```bash
# Ejemplo: Reiniciar Kafka en Master
ssh ec2-user@44.210.18.254
/opt/bigdata/kafka-3.6.0/bin/kafka-server-stop.sh
sleep 5
/opt/bigdata/kafka-3.6.0/bin/kafka-server-start.sh -daemon \
  /opt/bigdata/kafka-3.6.0/config/server.properties
```

---

## Costos y Recursos

### Costos Estimados (AWS Free Tier)

| Recurso | Costo/hora | Costo/dÃ­a (24h) | Costo/mes |
|---------|------------|-----------------|-----------|
| t3.small (Master) | $0.0208 | $0.50 | $15.00 |
| c7i-flex.large (Worker1) | $0.0880 | $2.11 | $63.36 |
| c7i-flex.large (Worker2) | $0.0880 | $2.11 | $63.36 |
| m7i-flex.large (Storage) | $0.0940 | $2.26 | $67.68 |
| EBS gp3 (120 GB) | $0.0100 | $0.24 | $7.20 |
| **TOTAL** | **$0.3008** | **$7.22** | **$216.60** |

**Estrategia de ahorro**:
- Apagar instancias fuera de horario de desarrollo
- Usar instance scheduler
- Considerar Spot Instances para Workers (70% descuento)
- Reducir volÃºmenes EBS si no se necesita todo el espacio

### Uso de Recursos Esperado

| MÃ©trica | Streaming (idle) | Streaming (activo) | Batch Processing |
|---------|------------------|-------------------|------------------|
| CPU Master | 10-20% | 40-60% | 30-50% |
| CPU Workers | 5-10% | 70-90% | 80-95% |
| RAM Master | 30-40% | 50-70% | 40-60% |
| RAM Workers | 20-30% | 60-80% | 70-90% |
| Network | 1-5 Mbps | 10-50 Mbps | 20-100 Mbps |
| Disk I/O | Bajo | Medio | Alto |

---

## Contacto y Soporte

**Equipo del Proyecto**:
- FabiÃ¡n
- Fernando
- Jorge

**Repositorio**: https://github.com/fernandoramirez1337/bigdata-pipeline

**Branch actual**: `claude/aws-ec2-distributed-plan-018mA2bSY4uvxcLBuHq1CkDS`

**DocumentaciÃ³n adicional**:
- [README.md](../README.md) - VisiÃ³n general del proyecto
- [PLAN_ARQUITECTURA.md](../PLAN_ARQUITECTURA.md) - DiseÃ±o detallado
- [END_TO_END_EXAMPLE.md](../END_TO_END_EXAMPLE.md) - Tutorial completo
- [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) - GuÃ­a de despliegue

---

**Ãšltima actualizaciÃ³n**: 20 de Noviembre 2025, 15:05 UTC
**Estado del cluster**: InstalaciÃ³n en progreso (Kafka downloading ~7%)
**PrÃ³xima acciÃ³n**: Esperar finalizaciÃ³n de `setup-all` (~25-35 minutos)
